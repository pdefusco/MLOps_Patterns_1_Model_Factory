{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9012761e",
   "metadata": {},
   "source": [
    "# Model Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f1b35",
   "metadata": {},
   "source": [
    "### This notebook shows how to create a baseline model pipeline and save it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0aed4",
   "metadata": {},
   "source": [
    "##### We save the Spark Dataframe as an Iceberg Table. Iceberg is a new open table format backed by Apple, Netflix and Cloudera. \n",
    "##### In the context of ML Ops, the most anticipated feature is Time Travel i.e. the ability to reproduce the data and the schema across different versions in time\n",
    "##### Finally, we create a simple PySpark pipeline and train a classifier with Keras/Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1246c56",
   "metadata": {},
   "source": [
    "* For a more comprehensive demo of Iceberg in CML, please visit the [Spark3 Iceberg CML Github Repository](https://github.com/pdefusco/Spark3_Iceberg_CML)\n",
    "* For a more detailed introduction to CML Session, Notebooks, and Spark tips and trips please visit the [CML Total Beginner GitHub Repository](https://github.com/pdefusco/CML-Total-Beginner)\n",
    "* For a more comprehensive example of the Atlas Python client mentioned below, please visit the [Atlas Client Example Notebook in the Data Integration with ML GitHub Repository](https://github.com/pdefusco/Data_Integration_wMachineLearning/blob/main/2_A_Atlas_Client_Example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbaee738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from sklearn.datasets import make_circles\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from helpers.plot_decision_boundary import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54236788-fce6-4633-9f80-98516bc8fe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting petastorm\n",
      "  Using cached petastorm-0.11.3-py2.py3-none-any.whl (283 kB)\n",
      "Collecting dill>=0.2.1\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Collecting future>=0.10.2\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "Requirement already satisfied: pyzmq>=14.0.0 in /usr/local/lib/python3.7/site-packages (from petastorm) (19.0.2)\n",
      "Collecting pyarrow>=0.17.1\n",
      "  Downloading pyarrow-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.6 MB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Using cached fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "Collecting diskcache>=3.0.0\n",
      "  Using cached diskcache-5.3.0-py3-none-any.whl (44 kB)\n",
      "Requirement already satisfied: pandas>=0.19.0 in ./.local/lib/python3.7/site-packages (from petastorm) (1.3.4)\n",
      "Requirement already satisfied: packaging>=15.0 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from petastorm) (21.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from petastorm) (1.19.4)\n",
      "Collecting pyspark>=2.1.0\n",
      "  Using cached pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from petastorm) (1.16.0)\n",
      "Collecting psutil>=4.0.0\n",
      "  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
      "\u001b[K     |████████████████████████████████| 296 kB 109.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from packaging>=15.0->petastorm) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from pandas>=0.19.0->petastorm) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from pandas>=0.19.0->petastorm) (2.8.2)\n",
      "Collecting py4j==0.10.9.2\n",
      "  Using cached py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "Building wheels for collected packages: future, pyspark\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=b38ac548f872c7df16598b5266fb85c07b824d62962b799c7f38e206e4b61fcb\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=a686bb367410b73802b915627544ea7673f707677505407d52ea395e55eeb0ce\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
      "Successfully built future pyspark\n",
      "Installing collected packages: py4j, pyspark, pyarrow, psutil, future, fsspec, diskcache, dill, petastorm\n",
      "Successfully installed dill-0.3.4 diskcache-5.3.0 fsspec-2021.11.1 future-0.18.2 petastorm-0.11.3 psutil-5.8.0 py4j-0.10.9.2 pyarrow-6.0.1 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install petastorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6efdef-6885-47de-96c2-c04340e5b622",
   "metadata": {},
   "source": [
    "#### The Spark Session is created with the following configurations. If you get an error, ensure your CML Session is using Runtimes and Spark 3.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714803d8-bc39-428e-9ed9-73c2e9e4c485",
   "metadata": {},
   "source": [
    "spark = SparkSession.builder.master('local[*]')\\\n",
    "  .config(\"spark.jars.packages\",\"org.apache.iceberg:iceberg-spark3-runtime:0.12.1\")\\\n",
    "  .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\\\n",
    "  .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.iceberg.spark.SparkSessionCatalog\")\\\n",
    "  .config(\"spark.sql.catalog.spark_catalog.type\",\"hive\")\\\n",
    "  .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-2\")\\\n",
    "  .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://gd01-uat2/\")\\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d797de-bc88-472d-a89e-72aeed7d75bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import requests\n",
    "\n",
    "def download_mnist_libsvm(mnist_data_dir):\n",
    "    mnist_data_path = os.path.join(mnist_data_dir, \"mnist.bz2\")\n",
    "    data_url = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2\"\n",
    "    r = requests.get(data_url)\n",
    "    with open(mnist_data_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "\n",
    "def get_mnist_dir():\n",
    "    # This folder is baked into the docker image\n",
    "    MNIST_DATA_DIR = \"/home/cdsw/data/mnist/\"\n",
    "\n",
    "    if os.path.isdir(MNIST_DATA_DIR) and os.path.isfile(os.path.join(MNIST_DATA_DIR, 'mnist.bz2')):\n",
    "        return MNIST_DATA_DIR\n",
    "\n",
    "    download_mnist_libsvm(MNIST_DATA_DIR)\n",
    "    return MNIST_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bc3273-9155-47a0-9235-5128e5e7fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dir = get_mnist_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d973ab09-d1cf-42e8-9cac-ed08bf1c460b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.7/site-packages/petastorm/spark/spark_dataset_converter.py:28: FutureWarning: pyarrow.LocalFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  from pyarrow import LocalFileSystem\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
    "\n",
    "try:\n",
    "    from pyspark.sql.functions import col\n",
    "except ImportError:\n",
    "    raise ImportError(\"This script runs with PySpark>=3.0.0\")\n",
    "\n",
    "\n",
    "def get_compiled_model(lr=0.001):\n",
    "    from tensorflow import keras\n",
    "\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(dataset, steps=1000, lr=0.001):\n",
    "    model = get_compiled_model(lr=lr)\n",
    "    model.fit(dataset, steps_per_epoch=steps)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "449f245d-f49d-42e0-a774-12b78422f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SparkSession\n",
    "spark = SparkSession.builder.master('local[*]')\\\n",
    "  .config(\"spark.jars.packages\",\"org.apache.iceberg:iceberg-spark3-runtime:0.12.1\")\\\n",
    "  .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\\\n",
    "  .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.iceberg.spark.SparkSessionCatalog\")\\\n",
    "  .config(\"spark.sql.catalog.spark_catalog.type\",\"hive\")\\\n",
    "  .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-2\")\\\n",
    "  .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://gd01-uat2/\")\\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dd021bac-4740-42c4-b3a1-f5bf9311d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data using Spark\n",
    "df = spark.read.format(\"libsvm\") \\\n",
    "    .option(\"numFeatures\", \"784\") \\\n",
    "    .load(mnist_dir) \\\n",
    "    .select(col(\"features\"), col(\"label\").cast(\"long\").alias(\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "156e79ad-2cac-40b5-b194-183446e7cce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(features,VectorUDT,true),StructField(label,LongType,true)))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415cb559-b112-43ab-a409-90ade3e25d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "da7ac930-d23d-4af8-abc3-97c07ae671da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[xs[0]: double, xs[1]: double, xs[2]: double, xs[3]: double, xs[4]: double, xs[5]: double, xs[6]: double, xs[7]: double, xs[8]: double, xs[9]: double, xs[10]: double, xs[11]: double, xs[12]: double, xs[13]: double, xs[14]: double, xs[15]: double, xs[16]: double, xs[17]: double, xs[18]: double, xs[19]: double, xs[20]: double, xs[21]: double, xs[22]: double, xs[23]: double, xs[24]: double, xs[25]: double, xs[26]: double, xs[27]: double, xs[28]: double, xs[29]: double, xs[30]: double, xs[31]: double, xs[32]: double, xs[33]: double, xs[34]: double, xs[35]: double, xs[36]: double, xs[37]: double, xs[38]: double, xs[39]: double, xs[40]: double, xs[41]: double, xs[42]: double, xs[43]: double, xs[44]: double, xs[45]: double, xs[46]: double, xs[47]: double, xs[48]: double, xs[49]: double, xs[50]: double, xs[51]: double, xs[52]: double, xs[53]: double, xs[54]: double, xs[55]: double, xs[56]: double, xs[57]: double, xs[58]: double, xs[59]: double, xs[60]: double, xs[61]: double, xs[62]: double, xs[63]: double, xs[64]: double, xs[65]: double, xs[66]: double, xs[67]: double, xs[68]: double, xs[69]: double, xs[70]: double, xs[71]: double, xs[72]: double, xs[73]: double, xs[74]: double, xs[75]: double, xs[76]: double, xs[77]: double, xs[78]: double, xs[79]: double, xs[80]: double, xs[81]: double, xs[82]: double, xs[83]: double, xs[84]: double, xs[85]: double, xs[86]: double, xs[87]: double, xs[88]: double, xs[89]: double, xs[90]: double, xs[91]: double, xs[92]: double, xs[93]: double, xs[94]: double, xs[95]: double, xs[96]: double, xs[97]: double, xs[98]: double, xs[99]: double, xs[100]: double, xs[101]: double, xs[102]: double, xs[103]: double, xs[104]: double, xs[105]: double, xs[106]: double, xs[107]: double, xs[108]: double, xs[109]: double, xs[110]: double, xs[111]: double, xs[112]: double, xs[113]: double, xs[114]: double, xs[115]: double, xs[116]: double, xs[117]: double, xs[118]: double, xs[119]: double, xs[120]: double, xs[121]: double, xs[122]: double, xs[123]: double, xs[124]: double, xs[125]: double, xs[126]: double, xs[127]: double, xs[128]: double, xs[129]: double, xs[130]: double, xs[131]: double, xs[132]: double, xs[133]: double, xs[134]: double, xs[135]: double, xs[136]: double, xs[137]: double, xs[138]: double, xs[139]: double, xs[140]: double, xs[141]: double, xs[142]: double, xs[143]: double, xs[144]: double, xs[145]: double, xs[146]: double, xs[147]: double, xs[148]: double, xs[149]: double, xs[150]: double, xs[151]: double, xs[152]: double, xs[153]: double, xs[154]: double, xs[155]: double, xs[156]: double, xs[157]: double, xs[158]: double, xs[159]: double, xs[160]: double, xs[161]: double, xs[162]: double, xs[163]: double, xs[164]: double, xs[165]: double, xs[166]: double, xs[167]: double, xs[168]: double, xs[169]: double, xs[170]: double, xs[171]: double, xs[172]: double, xs[173]: double, xs[174]: double, xs[175]: double, xs[176]: double, xs[177]: double, xs[178]: double, xs[179]: double, xs[180]: double, xs[181]: double, xs[182]: double, xs[183]: double, xs[184]: double, xs[185]: double, xs[186]: double, xs[187]: double, xs[188]: double, xs[189]: double, xs[190]: double, xs[191]: double, xs[192]: double, xs[193]: double, xs[194]: double, xs[195]: double, xs[196]: double, xs[197]: double, xs[198]: double, xs[199]: double, xs[200]: double, xs[201]: double, xs[202]: double, xs[203]: double, xs[204]: double, xs[205]: double, xs[206]: double, xs[207]: double, xs[208]: double, xs[209]: double, xs[210]: double, xs[211]: double, xs[212]: double, xs[213]: double, xs[214]: double, xs[215]: double, xs[216]: double, xs[217]: double, xs[218]: double, xs[219]: double, xs[220]: double, xs[221]: double, xs[222]: double, xs[223]: double, xs[224]: double, xs[225]: double, xs[226]: double, xs[227]: double, xs[228]: double, xs[229]: double, xs[230]: double, xs[231]: double, xs[232]: double, xs[233]: double, xs[234]: double, xs[235]: double, xs[236]: double, xs[237]: double, xs[238]: double, xs[239]: double, xs[240]: double, xs[241]: double, xs[242]: double, xs[243]: double, xs[244]: double, xs[245]: double, xs[246]: double, xs[247]: double, xs[248]: double, xs[249]: double, xs[250]: double, xs[251]: double, xs[252]: double, xs[253]: double, xs[254]: double, xs[255]: double, xs[256]: double, xs[257]: double, xs[258]: double, xs[259]: double, xs[260]: double, xs[261]: double, xs[262]: double, xs[263]: double, xs[264]: double, xs[265]: double, xs[266]: double, xs[267]: double, xs[268]: double, xs[269]: double, xs[270]: double, xs[271]: double, xs[272]: double, xs[273]: double, xs[274]: double, xs[275]: double, xs[276]: double, xs[277]: double, xs[278]: double, xs[279]: double, xs[280]: double, xs[281]: double, xs[282]: double, xs[283]: double, xs[284]: double, xs[285]: double, xs[286]: double, xs[287]: double, xs[288]: double, xs[289]: double, xs[290]: double, xs[291]: double, xs[292]: double, xs[293]: double, xs[294]: double, xs[295]: double, xs[296]: double, xs[297]: double, xs[298]: double, xs[299]: double, xs[300]: double, xs[301]: double, xs[302]: double, xs[303]: double, xs[304]: double, xs[305]: double, xs[306]: double, xs[307]: double, xs[308]: double, xs[309]: double, xs[310]: double, xs[311]: double, xs[312]: double, xs[313]: double, xs[314]: double, xs[315]: double, xs[316]: double, xs[317]: double, xs[318]: double, xs[319]: double, xs[320]: double, xs[321]: double, xs[322]: double, xs[323]: double, xs[324]: double, xs[325]: double, xs[326]: double, xs[327]: double, xs[328]: double, xs[329]: double, xs[330]: double, xs[331]: double, xs[332]: double, xs[333]: double, xs[334]: double, xs[335]: double, xs[336]: double, xs[337]: double, xs[338]: double, xs[339]: double, xs[340]: double, xs[341]: double, xs[342]: double, xs[343]: double, xs[344]: double, xs[345]: double, xs[346]: double, xs[347]: double, xs[348]: double, xs[349]: double, xs[350]: double, xs[351]: double, xs[352]: double, xs[353]: double, xs[354]: double, xs[355]: double, xs[356]: double, xs[357]: double, xs[358]: double, xs[359]: double, xs[360]: double, xs[361]: double, xs[362]: double, xs[363]: double, xs[364]: double, xs[365]: double, xs[366]: double, xs[367]: double, xs[368]: double, xs[369]: double, xs[370]: double, xs[371]: double, xs[372]: double, xs[373]: double, xs[374]: double, xs[375]: double, xs[376]: double, xs[377]: double, xs[378]: double, xs[379]: double, xs[380]: double, xs[381]: double, xs[382]: double, xs[383]: double, xs[384]: double, xs[385]: double, xs[386]: double, xs[387]: double, xs[388]: double, xs[389]: double, xs[390]: double, xs[391]: double, xs[392]: double, xs[393]: double, xs[394]: double, xs[395]: double, xs[396]: double, xs[397]: double, xs[398]: double, xs[399]: double, xs[400]: double, xs[401]: double, xs[402]: double, xs[403]: double, xs[404]: double, xs[405]: double, xs[406]: double, xs[407]: double, xs[408]: double, xs[409]: double, xs[410]: double, xs[411]: double, xs[412]: double, xs[413]: double, xs[414]: double, xs[415]: double, xs[416]: double, xs[417]: double, xs[418]: double, xs[419]: double, xs[420]: double, xs[421]: double, xs[422]: double, xs[423]: double, xs[424]: double, xs[425]: double, xs[426]: double, xs[427]: double, xs[428]: double, xs[429]: double, xs[430]: double, xs[431]: double, xs[432]: double, xs[433]: double, xs[434]: double, xs[435]: double, xs[436]: double, xs[437]: double, xs[438]: double, xs[439]: double, xs[440]: double, xs[441]: double, xs[442]: double, xs[443]: double, xs[444]: double, xs[445]: double, xs[446]: double, xs[447]: double, xs[448]: double, xs[449]: double, xs[450]: double, xs[451]: double, xs[452]: double, xs[453]: double, xs[454]: double, xs[455]: double, xs[456]: double, xs[457]: double, xs[458]: double, xs[459]: double, xs[460]: double, xs[461]: double, xs[462]: double, xs[463]: double, xs[464]: double, xs[465]: double, xs[466]: double, xs[467]: double, xs[468]: double, xs[469]: double, xs[470]: double, xs[471]: double, xs[472]: double, xs[473]: double, xs[474]: double, xs[475]: double, xs[476]: double, xs[477]: double, xs[478]: double, xs[479]: double, xs[480]: double, xs[481]: double, xs[482]: double, xs[483]: double, xs[484]: double, xs[485]: double, xs[486]: double, xs[487]: double, xs[488]: double, xs[489]: double, xs[490]: double, xs[491]: double, xs[492]: double, xs[493]: double, xs[494]: double, xs[495]: double, xs[496]: double, xs[497]: double, xs[498]: double, xs[499]: double, xs[500]: double, xs[501]: double, xs[502]: double, xs[503]: double, xs[504]: double, xs[505]: double, xs[506]: double, xs[507]: double, xs[508]: double, xs[509]: double, xs[510]: double, xs[511]: double, xs[512]: double, xs[513]: double, xs[514]: double, xs[515]: double, xs[516]: double, xs[517]: double, xs[518]: double, xs[519]: double, xs[520]: double, xs[521]: double, xs[522]: double, xs[523]: double, xs[524]: double, xs[525]: double, xs[526]: double, xs[527]: double, xs[528]: double, xs[529]: double, xs[530]: double, xs[531]: double, xs[532]: double, xs[533]: double, xs[534]: double, xs[535]: double, xs[536]: double, xs[537]: double, xs[538]: double, xs[539]: double, xs[540]: double, xs[541]: double, xs[542]: double, xs[543]: double, xs[544]: double, xs[545]: double, xs[546]: double, xs[547]: double, xs[548]: double, xs[549]: double, xs[550]: double, xs[551]: double, xs[552]: double, xs[553]: double, xs[554]: double, xs[555]: double, xs[556]: double, xs[557]: double, xs[558]: double, xs[559]: double, xs[560]: double, xs[561]: double, xs[562]: double, xs[563]: double, xs[564]: double, xs[565]: double, xs[566]: double, xs[567]: double, xs[568]: double, xs[569]: double, xs[570]: double, xs[571]: double, xs[572]: double, xs[573]: double, xs[574]: double, xs[575]: double, xs[576]: double, xs[577]: double, xs[578]: double, xs[579]: double, xs[580]: double, xs[581]: double, xs[582]: double, xs[583]: double, xs[584]: double, xs[585]: double, xs[586]: double, xs[587]: double, xs[588]: double, xs[589]: double, xs[590]: double, xs[591]: double, xs[592]: double, xs[593]: double, xs[594]: double, xs[595]: double, xs[596]: double, xs[597]: double, xs[598]: double, xs[599]: double, xs[600]: double, xs[601]: double, xs[602]: double, xs[603]: double, xs[604]: double, xs[605]: double, xs[606]: double, xs[607]: double, xs[608]: double, xs[609]: double, xs[610]: double, xs[611]: double, xs[612]: double, xs[613]: double, xs[614]: double, xs[615]: double, xs[616]: double, xs[617]: double, xs[618]: double, xs[619]: double, xs[620]: double, xs[621]: double, xs[622]: double, xs[623]: double, xs[624]: double, xs[625]: double, xs[626]: double, xs[627]: double, xs[628]: double, xs[629]: double, xs[630]: double, xs[631]: double, xs[632]: double, xs[633]: double, xs[634]: double, xs[635]: double, xs[636]: double, xs[637]: double, xs[638]: double, xs[639]: double, xs[640]: double, xs[641]: double, xs[642]: double, xs[643]: double, xs[644]: double, xs[645]: double, xs[646]: double, xs[647]: double, xs[648]: double, xs[649]: double, xs[650]: double, xs[651]: double, xs[652]: double, xs[653]: double, xs[654]: double, xs[655]: double, xs[656]: double, xs[657]: double, xs[658]: double, xs[659]: double, xs[660]: double, xs[661]: double, xs[662]: double, xs[663]: double, xs[664]: double, xs[665]: double, xs[666]: double, xs[667]: double, xs[668]: double, xs[669]: double, xs[670]: double, xs[671]: double, xs[672]: double, xs[673]: double, xs[674]: double, xs[675]: double, xs[676]: double, xs[677]: double, xs[678]: double, xs[679]: double, xs[680]: double, xs[681]: double, xs[682]: double, xs[683]: double, xs[684]: double, xs[685]: double, xs[686]: double, xs[687]: double, xs[688]: double, xs[689]: double, xs[690]: double, xs[691]: double, xs[692]: double, xs[693]: double, xs[694]: double, xs[695]: double, xs[696]: double, xs[697]: double, xs[698]: double, xs[699]: double, xs[700]: double, xs[701]: double, xs[702]: double, xs[703]: double, xs[704]: double, xs[705]: double, xs[706]: double, xs[707]: double, xs[708]: double, xs[709]: double, xs[710]: double, xs[711]: double, xs[712]: double, xs[713]: double, xs[714]: double, xs[715]: double, xs[716]: double, xs[717]: double, xs[718]: double, xs[719]: double, xs[720]: double, xs[721]: double, xs[722]: double, xs[723]: double, xs[724]: double, xs[725]: double, xs[726]: double, xs[727]: double, xs[728]: double, xs[729]: double, xs[730]: double, xs[731]: double, xs[732]: double, xs[733]: double, xs[734]: double, xs[735]: double, xs[736]: double, xs[737]: double, xs[738]: double, xs[739]: double, xs[740]: double, xs[741]: double, xs[742]: double, xs[743]: double, xs[744]: double, xs[745]: double, xs[746]: double, xs[747]: double, xs[748]: double, xs[749]: double, xs[750]: double, xs[751]: double, xs[752]: double, xs[753]: double, xs[754]: double, xs[755]: double, xs[756]: double, xs[757]: double, xs[758]: double, xs[759]: double, xs[760]: double, xs[761]: double, xs[762]: double, xs[763]: double, xs[764]: double, xs[765]: double, xs[766]: double, xs[767]: double, xs[768]: double, xs[769]: double, xs[770]: double, xs[771]: double, xs[772]: double, xs[773]: double, xs[774]: double, xs[775]: double, xs[776]: double, xs[777]: double, xs[778]: double, xs[779]: double, xs[780]: double, xs[781]: double, xs[782]: double, xs[783]: double]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "df.withColumn(\"xs\", vector_to_array(\"features\")).select([col(\"xs\")[i] for i in range(784)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d404f3-9932-42fb-b01b-b50a7e12363f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9db939fa-53ae-4fea-b2b1-4f7229b09f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE new_ice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5a96f260-6009-4074-8121-09755fcceeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the Spark Dataframe as an Iceberg table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS new_ice (features struct<type:tinyint,size:int,indices:array<int>,values:array<double>>, label BIGINT) USING iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "00334d59-dba4-4388-9de0-04e53749cc45",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot write to 'spark_catalog.default.new_ice', too many data columns:\nTable columns: 'features'\nData columns: 'features', 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-1d3d00f86abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iceberg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"default.new_ice\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot write to 'spark_catalog.default.new_ice', too many data columns:\nTable columns: 'features'\nData columns: 'features', 'label'"
     ]
    }
   ],
   "source": [
    "df.write.format(\"iceberg\").mode(\"overwrite\").save(\"default.new_ice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87f21b-879d-46ec-869a-9b608eb31a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"iceberg\").load(\"default.new_ice.snapshots\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "943271c1-b63b-474e-a427-f648af1fa671",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af36ab-cc86-43d8-adcc-374bfcd07dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f84f68-7783-413e-8758-e01141f65613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ecfcd52-2cb7-463a-ac4a-040be6b9a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeTo(\"spark_catalog.default.mnist_iceberg_cml\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b24b3dbe-b437-4022-b026-a75936daa90d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table default.mnist_iceberg_cml.snapshots not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-2cf6ad119aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iceberg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"default.mnist_iceberg_cml.snapshots\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table default.mnist_iceberg_cml.snapshots not found"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"iceberg\").load(\"default.mnist_iceberg_cml.snapshots\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00726986-3265-4cc4-be25-02ad8b86c16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94425b6e-15d2-490b-8f03-6e26f956ea9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o387.showString.\n: java.io.IOException: Can't get Master Kerberos principal for use as renewer\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:134)\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:102)\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:81)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:217)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:328)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:442)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-d524982fad37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from default.mnist_iceberg_cml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o387.showString.\n: java.io.IOException: Can't get Master Kerberos principal for use as renewer\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:134)\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:102)\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:81)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:217)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:328)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:442)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from default.mnist_iceberg_cml\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d80453-5082-4716-9b8f-43aed79ffaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ddbb1a7-d11d-41a2-b44c-0dcafb3fdb58",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot write incompatible data to table 'spark_catalog.default.mnist_iceberg':\n- Cannot write 'features': struct<type:tinyint,size:int,indices:array<int>,values:array<double>> is incompatible with string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c134dc08d4ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE TABLE IF NOT EXISTS ice_cml (features string, label bigint) USING iceberg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iceberg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"default.mnist_iceberg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot write incompatible data to table 'spark_catalog.default.mnist_iceberg':\n- Cannot write 'features': struct<type:tinyint,size:int,indices:array<int>,values:array<double>> is incompatible with string"
     ]
    }
   ],
   "source": [
    "# Saving the Spark Dataframe as an Iceberg table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS ice_cml (features string, label bigint) USING iceberg\")\n",
    "\n",
    "df.write.format(\"iceberg\").mode(\"overwrite\").save(\"default.mnist_iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "516e0a96-a5ba-45a1-8449-8d4374d254e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6a4745c-dfac-43d2-8055-8ea4d45c20ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.7/site-packages/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  self._filesystem = pyarrow.localfs\n",
      "Converting floating-point columns to float32\n",
      "The median size 10765737 B (< 50 MB) of the parquet files is too small. Total size: 16658412 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///tmp/petastorm/cache/tf-example/20211204003041-appid-local-1638577702563-7cae4a6d-21d7-49ea-aa1b-7163ea42d401/part-00001-036bb370-60b7-4c80-931f-f84d2c303dbd-c000.parquet, ...\n",
      "Converting floating-point columns to float32\n",
      "The median size 1231397 B (< 50 MB) of the parquet files is too small. Total size: 1903223 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///tmp/petastorm/cache/tf-example/20211204003051-appid-local-1638577702563-dffa2649-1747-4b1a-8996-520243496975/part-00001-ae58eb09-37df-46fc-8aa2-997f1ee352ee-c000.parquet, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 1ms/step - loss: 4.4996 - accuracy: 0.6770\n",
      "193/193 [==============================] - 1s 3ms/step - loss: 1.2330 - accuracy: 0.6653\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Randomly split data into train and test dataset\n",
    "df_train, df_test = df.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "# Set a cache directory for intermediate data.\n",
    "# The path should be accessible by both Spark workers and driver.\n",
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF,\n",
    "               \"file:///tmp/petastorm/cache/tf-example\")\n",
    "\n",
    "converter_train = make_spark_converter(df_train)\n",
    "converter_test = make_spark_converter(df_test)\n",
    "\n",
    "def train_and_evaluate(_=None):\n",
    "    import tensorflow.compat.v1 as tf  # pylint: disable=import-error\n",
    "\n",
    "    with converter_train.make_tf_dataset() as dataset:\n",
    "        dataset = dataset.map(lambda x: (tf.reshape(x.features, [-1, 28, 28]), x.label))\n",
    "        model = train(dataset)\n",
    "\n",
    "    with converter_test.make_tf_dataset(num_epochs=1) as dataset:\n",
    "        dataset = dataset.map(lambda x: (tf.reshape(x.features, [-1, 28, 28]), x.label))\n",
    "        hist = model.evaluate(dataset)\n",
    "\n",
    "    return hist[1]\n",
    "\n",
    "# Train and evaluate the model on the local machine\n",
    "accuracy = train_and_evaluate()\n",
    "logging.info(\"Train and evaluate the model on the local machine.\")\n",
    "logging.info(\"Accuracy: %.6f\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6e6971f-08f1-4372-b3b5-a92108aafd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6653121113777161"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd9a457-a3f2-42a2-9503-596cfd429fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Train and evaluate the model remotely on a spark worker, \"\n",
    "             \"which can be used for distributed hyperparameter tuning.\")\n",
    "logging.info(\"Accuracy: %.6f\", accuracy)\n",
    "\n",
    "# Cleanup\n",
    "converter_train.delete()\n",
    "converter_test.delete()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f08e0e-b080-4185-9771-650cfbf89425",
   "metadata": {},
   "source": [
    "#### Just some fake data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095723d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.754246</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.756159</td>\n",
       "      <td>0.153259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.815392</td>\n",
       "      <td>0.173282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.393731</td>\n",
       "      <td>0.692883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.442208</td>\n",
       "      <td>-0.896723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       var1      var2  label\n",
       "0  0.754246  0.231481      1\n",
       "1 -0.756159  0.153259      1\n",
       "2 -0.815392  0.173282      1\n",
       "3 -0.393731  0.692883      1\n",
       "4  0.442208 -0.896723      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make 1000 examples\n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_circles(n_samples, \n",
    "                    noise=0.03, \n",
    "                    random_state=42)\n",
    "\n",
    "circles = pd.DataFrame({\"var1\":X[:, 0], \"var2\":X[:, 1], \"label\":y})\n",
    "circles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd40cd0-af1e-4f84-868f-00b0ffcb6a32",
   "metadata": {},
   "source": [
    "#### We can save the DataFrame as an Iceberg Table using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f2ca2e-1f94-4524-b206-8c838b207424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark Dataframe from the Pandas Dataframe\n",
    "sparkDF=spark.createDataFrame(circles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b77b4951-fb99-4ab7-b854-74c1cdcd6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Spark Dataframe as an Iceberg table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS ice_cml (var1 int, var2 int, label int) USING iceberg\")\n",
    "\n",
    "sparkDF.write.format(\"iceberg\").mode(\"overwrite\").save(\"default.ice_cml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc67509-f906-40fd-af65-87d5036673dc",
   "metadata": {},
   "source": [
    "#### The table is automatically tracked by the Data Lake associated with the CML Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85cb1ce-e595-45c1-b080-40c3accbd0b6",
   "metadata": {},
   "source": [
    "#### To check that a new entry for the table has been added to Atlas in the Data Lake, go back to the CDP Homepage and open Data Catalog. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6bc771-6981-4472-a781-49d651144da8",
   "metadata": {},
   "source": [
    "#### Select the Data Lake (i.e. Cloud Environment) that your worskpace was built in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015a62d-5ce3-492d-90d2-7ff9e51979b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50fc4b88-e6ac-4555-b680-a7bca4cee876",
   "metadata": {},
   "source": [
    "#### Use the Atlas Search bar at the top to browse for the table and click on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6429de-8a14-413f-9ee5-d8f8ce6ce603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "835f11ac-83a2-4ef8-b19b-cbc0b5595602",
   "metadata": {},
   "source": [
    "#### Notice Atlas is tracking a lot of interesting Metadata including Table Attributes, Lineage, and a lot More. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef1b4f-41b2-4b8c-88ba-65b377eaf897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21d11d0e-6d5b-4c00-9cd2-f14994758040",
   "metadata": {},
   "source": [
    "#### The Metadata can even be customized. [This notebook](https://github.com/pdefusco/Data_Integration_wMachineLearning/blob/main/2_A_Atlas_Client_Example.ipynb) shows how you can use the Atlas Python Client to build custom lineage flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62128e78-ec09-4963-af8a-434f01497c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81838f58-3139-40d9-87d9-a824fc740440",
   "metadata": {},
   "source": [
    "#### Back to Modeling. We will use Keras and Tensorflow to build this classifier. Our data is in Spark though, so we will use Petastorm to transform the data Tensorflow-readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5f64479-85fd-40b6-980e-6eb594e49464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting petastorm\n",
      "  Downloading petastorm-0.11.3-py2.py3-none-any.whl (283 kB)\n",
      "\u001b[K     |████████████████████████████████| 283 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/site-packages (from petastorm) (1.19.4)\n",
      "Collecting psutil>=4.0.0\n",
      "  Downloading psutil-5.8.0-cp36-cp36m-manylinux2010_x86_64.whl (291 kB)\n",
      "\u001b[K     |████████████████████████████████| 291 kB 92.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.19.0 in ./.local/lib/python3.6/site-packages (from petastorm) (1.1.5)\n",
      "Collecting future>=0.10.2\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 113.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 101.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting diskcache>=3.0.0\n",
      "  Downloading diskcache-5.3.0-py3-none-any.whl (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 451 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=15.0 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from petastorm) (21.0)\n",
      "Collecting pyarrow>=0.17.1\n",
      "  Downloading pyarrow-6.0.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.6 MB 116.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyzmq>=14.0.0 in /usr/local/lib/python3.6/site-packages (from petastorm) (19.0.2)\n",
      "Collecting dill>=0.2.1\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 880 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting pyspark>=2.1.0\n",
      "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████    | 246.9 MB 96.6 MB/s eta 0:00:011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 281.3 MB 34 kB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from petastorm) (1.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from packaging>=15.0->petastorm) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from pandas>=0.19.0->petastorm) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from pandas>=0.19.0->petastorm) (2021.3)\n",
      "Collecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 110.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: future, pyspark\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=32fda17f62ac6e87eaef8a6e235595e4c7024d506fdcc3aaba3a14d39688e0fc\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/6e/9c/ed/4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=1b9e3b29a52b66939ba7f5befa8ff94dbd56c39dcfbe7b8fc7924fe89509f65e\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/e8/d9/e5/78436a0a3899d81410aeb45b200153113667f2e250f6882ada\n",
      "Successfully built future pyspark\n",
      "Installing collected packages: py4j, pyspark, pyarrow, psutil, future, fsspec, diskcache, dill, petastorm\n",
      "Successfully installed dill-0.3.4 diskcache-5.3.0 fsspec-2021.11.1 future-0.18.2 petastorm-0.11.3 psutil-5.8.0 py4j-0.10.9.2 pyarrow-6.0.1 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3e3b0c9-4416-4848-9f22-f8e37b000978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
    "import tensorflow.compat.v1 as tf  # pylint: disable=import-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "293756f5-2346-40a8-bbc1-35c994a9cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a cache dir first.\n",
    "\n",
    "# Set a cache directory for intermediate data.\n",
    "# The path should be accessible by both Spark workers and driver.\n",
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF,\"file:///tmp/petastorm/cache/tf-example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0c17f-7d10-4014-b93a-b7b5e3233502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60f5b076-ee95-49fb-8105-5c41ead4a429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting floating-point columns to float32\n",
      "The median size 5033 B (< 50 MB) of the parquet files is too small. Total size: 10062 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///tmp/petastorm/cache/tf-example/20211203233933-appid-local-1638570042715-2c963542-b1cd-4cd6-8bf5-1297cbb97040/part-00000-db00b0de-8eba-40c6-8675-01b28dff976b-c000.parquet, ...\n"
     ]
    }
   ],
   "source": [
    "# create a converter from `df`\n",
    "# it will materialize `df` to cache dir.\n",
    "converter = make_spark_converter(sparkDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7bd49-8ac7-4047-9a73-d9dd82dc4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dff008b7-bd09-4555-a00b-f6aa5735dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model (same as model_7)\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(4, activation=\"relu\"), # hidden layer 1, using \"relu\" for activation (same as tf.keras.activations.relu)\n",
    "  tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(1, activation=\"sigmoid\") # output layer, using 'sigmoid' for the output\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "017d7c69-e414-4946-b212-01bf6a5afb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), # increase learning rate from 0.001 to 0.01 for faster learning\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d548d6e-cdd3-4573-8937-7b0a4a8a41fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 607779/Unknown - 524s 860us/step - loss: -900.7520 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "# make a tensorflow dataset from `converter`\n",
    "with converter.make_tf_dataset() as dataset:\n",
    "    # the `dataset` is `tf.data.Dataset` object\n",
    "    # we can train/evaluate model on the `dataset`\n",
    "    history = model.fit(dataset)\n",
    "    # when exiting the context, the reader of the dataset will be closed\n",
    "    \n",
    "    # Evaluate our model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Model loss on the test set: {loss}\")\n",
    "    print(f\"Model accuracy on the test set: {100*accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# delete the cached files of the dataframe.\n",
    "converter.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with a plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu);\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, y_train = X[:800], y[:800] # 80% of the data for the training set\n",
    "X_test, y_test = X[800:], y[800:] # 20% of the data for the test set\n",
    "\n",
    "# Check the shapes of the data\n",
    "X_train.shape, X_test.shape # 800 examples in the training set, 200 examples in the test set\n",
    "\n",
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create the model (same as model_7)\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(4, activation=\"relu\"), # hidden layer 1, using \"relu\" for activation (same as tf.keras.activations.relu)\n",
    "  tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(1, activation=\"sigmoid\") # output layer, using 'sigmoid' for the output\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "                optimizer=tf.keras.optimizers.Adam(lr=0.01), # increase learning rate from 0.001 to 0.01 for faster learning\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=25)\n",
    "\n",
    "# Evaluate our model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Model loss on the test set: {loss}\")\n",
    "print(f\"Model accuracy on the test set: {100*accuracy:.2f}%\")\n",
    "\n",
    "# Plot the decision boundaries for the training and test sets\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model, X=X_train, y=y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model, X=X_test, y=y_test)\n",
    "plt.show()\n",
    "\n",
    "# You can access the information in the history variable using the .history attribute\n",
    "pd.DataFrame(history.history)\n",
    "\n",
    "# Plot the loss curves\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.title(\"Model training curves\")\n",
    "\n",
    "\n",
    "model.save('models/my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
