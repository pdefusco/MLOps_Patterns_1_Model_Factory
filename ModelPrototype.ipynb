{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9012761e",
   "metadata": {},
   "source": [
    "# Model Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f1b35",
   "metadata": {},
   "source": [
    "### This notebook shows how to create a baseline model pipeline and save it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0aed4",
   "metadata": {},
   "source": [
    "##### We save the Spark Dataframe as an Iceberg Table. Iceberg is a new open table format backed by Apple, Netflix and Cloudera. \n",
    "##### In the context of ML Ops, the most anticipated feature is Time Travel i.e. the ability to reproduce the data and the schema across different versions in time\n",
    "##### Finally, we create a simple PySpark pipeline and train a classifier with Keras/Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1246c56",
   "metadata": {},
   "source": [
    "* For a more comprehensive demo of Iceberg in CML, please visit the [Spark3 Iceberg CML Github Repository](https://github.com/pdefusco/Spark3_Iceberg_CML)\n",
    "* For a more detailed introduction to CML Session, Notebooks, and Spark tips and trips please visit the [CML Total Beginner GitHub Repository](https://github.com/pdefusco/CML-Total-Beginner)\n",
    "* For a more comprehensive example of the Atlas Python client mentioned below, please visit the [Atlas Client Example Notebook in the Data Integration with ML GitHub Repository](https://github.com/pdefusco/Data_Integration_wMachineLearning/blob/main/2_A_Atlas_Client_Example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbaee738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from sklearn.datasets import make_circles\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from helpers.plot_decision_boundary import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6efdef-6885-47de-96c2-c04340e5b622",
   "metadata": {},
   "source": [
    "#### The Spark Session is created with the following configurations. If you get an error, ensure your CML Session is using Runtimes and Spark 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef66ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[*]')\\\n",
    "  .config(\"spark.jars.packages\",\"org.apache.iceberg:iceberg-spark3-runtime:0.12.1\")\\\n",
    "  .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\\\n",
    "  .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.iceberg.spark.SparkSessionCatalog\")\\\n",
    "  .config(\"spark.sql.catalog.spark_catalog.type\",\"hive\")\\\n",
    "  .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-2\")\\\n",
    "  .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://gd01-uat2/\")\\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f08e0e-b080-4185-9771-650cfbf89425",
   "metadata": {},
   "source": [
    "#### Just some fake data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095723d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.754246</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.756159</td>\n",
       "      <td>0.153259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.815392</td>\n",
       "      <td>0.173282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.393731</td>\n",
       "      <td>0.692883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.442208</td>\n",
       "      <td>-0.896723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       var1      var2  label\n",
       "0  0.754246  0.231481      1\n",
       "1 -0.756159  0.153259      1\n",
       "2 -0.815392  0.173282      1\n",
       "3 -0.393731  0.692883      1\n",
       "4  0.442208 -0.896723      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make 1000 examples\n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_circles(n_samples, \n",
    "                    noise=0.03, \n",
    "                    random_state=42)\n",
    "\n",
    "circles = pd.DataFrame({\"var1\":X[:, 0], \"var2\":X[:, 1], \"label\":y})\n",
    "circles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd40cd0-af1e-4f84-868f-00b0ffcb6a32",
   "metadata": {},
   "source": [
    "#### We can save the DataFrame as an Iceberg Table using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f2ca2e-1f94-4524-b206-8c838b207424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark Dataframe from the Pandas Dataframe\n",
    "sparkDF=spark.createDataFrame(circles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b77b4951-fb99-4ab7-b854-74c1cdcd6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Spark Dataframe as an Iceberg table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS ice_cml (var1 int, var2 int, label int) USING iceberg\")\n",
    "\n",
    "sparkDF.write.format(\"iceberg\").mode(\"overwrite\").save(\"default.ice_cml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc67509-f906-40fd-af65-87d5036673dc",
   "metadata": {},
   "source": [
    "#### The table is automatically tracked by the Data Lake associated with the CML Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85cb1ce-e595-45c1-b080-40c3accbd0b6",
   "metadata": {},
   "source": [
    "#### To check that a new entry for the table has been added to Atlas in the Data Lake, go back to the CDP Homepage and open Data Catalog. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6bc771-6981-4472-a781-49d651144da8",
   "metadata": {},
   "source": [
    "#### Select the Data Lake (i.e. Cloud Environment) that your worskpace was built in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015a62d-5ce3-492d-90d2-7ff9e51979b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50fc4b88-e6ac-4555-b680-a7bca4cee876",
   "metadata": {},
   "source": [
    "#### Use the Atlas Search bar at the top to browse for the table and click on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6429de-8a14-413f-9ee5-d8f8ce6ce603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "835f11ac-83a2-4ef8-b19b-cbc0b5595602",
   "metadata": {},
   "source": [
    "#### Notice Atlas is tracking a lot of interesting Metadata including Table Attributes, Lineage, and a lot More. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef1b4f-41b2-4b8c-88ba-65b377eaf897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21d11d0e-6d5b-4c00-9cd2-f14994758040",
   "metadata": {},
   "source": [
    "#### The Metadata can even be customized. [This notebook](https://github.com/pdefusco/Data_Integration_wMachineLearning/blob/main/2_A_Atlas_Client_Example.ipynb) shows how you can use the Atlas Python Client to build custom lineage flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62128e78-ec09-4963-af8a-434f01497c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81838f58-3139-40d9-87d9-a824fc740440",
   "metadata": {},
   "source": [
    "#### Back to Modeling. We will use Keras and Tensorflow to build this classifier. Our data is in Spark though, so we will use Petastorm to transform the data Tensorflow-readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5f64479-85fd-40b6-980e-6eb594e49464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting petastorm\n",
      "  Downloading petastorm-0.11.3-py2.py3-none-any.whl (283 kB)\n",
      "\u001b[K     |████████████████████████████████| 283 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/site-packages (from petastorm) (1.19.4)\n",
      "Collecting psutil>=4.0.0\n",
      "  Downloading psutil-5.8.0-cp36-cp36m-manylinux2010_x86_64.whl (291 kB)\n",
      "\u001b[K     |████████████████████████████████| 291 kB 92.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.19.0 in ./.local/lib/python3.6/site-packages (from petastorm) (1.1.5)\n",
      "Collecting future>=0.10.2\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 113.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 101.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting diskcache>=3.0.0\n",
      "  Downloading diskcache-5.3.0-py3-none-any.whl (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 451 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=15.0 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from petastorm) (21.0)\n",
      "Collecting pyarrow>=0.17.1\n",
      "  Downloading pyarrow-6.0.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.6 MB 116.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyzmq>=14.0.0 in /usr/local/lib/python3.6/site-packages (from petastorm) (19.0.2)\n",
      "Collecting dill>=0.2.1\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 880 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting pyspark>=2.1.0\n",
      "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████    | 246.9 MB 96.6 MB/s eta 0:00:011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 281.3 MB 34 kB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from petastorm) (1.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from packaging>=15.0->petastorm) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from pandas>=0.19.0->petastorm) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from pandas>=0.19.0->petastorm) (2021.3)\n",
      "Collecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 110.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: future, pyspark\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=32fda17f62ac6e87eaef8a6e235595e4c7024d506fdcc3aaba3a14d39688e0fc\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/6e/9c/ed/4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=1b9e3b29a52b66939ba7f5befa8ff94dbd56c39dcfbe7b8fc7924fe89509f65e\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/e8/d9/e5/78436a0a3899d81410aeb45b200153113667f2e250f6882ada\n",
      "Successfully built future pyspark\n",
      "Installing collected packages: py4j, pyspark, pyarrow, psutil, future, fsspec, diskcache, dill, petastorm\n",
      "Successfully installed dill-0.3.4 diskcache-5.3.0 fsspec-2021.11.1 future-0.18.2 petastorm-0.11.3 psutil-5.8.0 py4j-0.10.9.2 pyarrow-6.0.1 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3e3b0c9-4416-4848-9f22-f8e37b000978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
    "import tensorflow.compat.v1 as tf  # pylint: disable=import-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "293756f5-2346-40a8-bbc1-35c994a9cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a cache dir first.\n",
    "\n",
    "# Set a cache directory for intermediate data.\n",
    "# The path should be accessible by both Spark workers and driver.\n",
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF,\"file:///tmp/petastorm/cache/tf-example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0c17f-7d10-4014-b93a-b7b5e3233502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60f5b076-ee95-49fb-8105-5c41ead4a429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting floating-point columns to float32\n",
      "The median size 5033 B (< 50 MB) of the parquet files is too small. Total size: 10062 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///tmp/petastorm/cache/tf-example/20211203233933-appid-local-1638570042715-2c963542-b1cd-4cd6-8bf5-1297cbb97040/part-00000-db00b0de-8eba-40c6-8675-01b28dff976b-c000.parquet, ...\n"
     ]
    }
   ],
   "source": [
    "# create a converter from `df`\n",
    "# it will materialize `df` to cache dir.\n",
    "converter = make_spark_converter(sparkDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7bd49-8ac7-4047-9a73-d9dd82dc4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dff008b7-bd09-4555-a00b-f6aa5735dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model (same as model_7)\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(4, activation=\"relu\"), # hidden layer 1, using \"relu\" for activation (same as tf.keras.activations.relu)\n",
    "  tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(1, activation=\"sigmoid\") # output layer, using 'sigmoid' for the output\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "017d7c69-e414-4946-b212-01bf6a5afb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), # increase learning rate from 0.001 to 0.01 for faster learning\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d548d6e-cdd3-4573-8937-7b0a4a8a41fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 727007/Unknown - 626s 859us/step - loss: -1534.6987 - accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ac627a2b2b48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# the `dataset` is `tf.data.Dataset` object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# we can train/evaluate model on the `dataset`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# when exiting the context, the reader of the dataset will be closed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# make a tensorflow dataset from `converter`\n",
    "with converter.make_tf_dataset() as dataset:\n",
    "    # the `dataset` is `tf.data.Dataset` object\n",
    "    # we can train/evaluate model on the `dataset`\n",
    "    history = model.fit(dataset)\n",
    "    # when exiting the context, the reader of the dataset will be closed\n",
    "    \n",
    "    # Evaluate our model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Model loss on the test set: {loss}\")\n",
    "    print(f\"Model accuracy on the test set: {100*accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# delete the cached files of the dataframe.\n",
    "converter.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebb856-654d-411e-8b82-05604e407b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualize with a plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu);\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, y_train = X[:800], y[:800] # 80% of the data for the training set\n",
    "X_test, y_test = X[800:], y[800:] # 20% of the data for the test set\n",
    "\n",
    "# Check the shapes of the data\n",
    "X_train.shape, X_test.shape # 800 examples in the training set, 200 examples in the test set\n",
    "\n",
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create the model (same as model_7)\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(4, activation=\"relu\"), # hidden layer 1, using \"relu\" for activation (same as tf.keras.activations.relu)\n",
    "  tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(1, activation=\"sigmoid\") # output layer, using 'sigmoid' for the output\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "                optimizer=tf.keras.optimizers.Adam(lr=0.01), # increase learning rate from 0.001 to 0.01 for faster learning\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=25)\n",
    "\n",
    "# Evaluate our model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Model loss on the test set: {loss}\")\n",
    "print(f\"Model accuracy on the test set: {100*accuracy:.2f}%\")\n",
    "\n",
    "# Plot the decision boundaries for the training and test sets\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model, X=X_train, y=y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model, X=X_test, y=y_test)\n",
    "plt.show()\n",
    "\n",
    "# You can access the information in the history variable using the .history attribute\n",
    "pd.DataFrame(history.history)\n",
    "\n",
    "# Plot the loss curves\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.title(\"Model training curves\")\n",
    "\n",
    "\n",
    "model.save('models/my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
